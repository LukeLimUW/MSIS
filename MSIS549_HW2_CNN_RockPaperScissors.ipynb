{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MSIS579-HW2-CNN-RockPaperScissors.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVxez4Gbwh--",
        "colab_type": "text"
      },
      "source": [
        "# MSIS 579 HW2 CNN to recognize Rock/Paper/Scissors\n",
        "\n",
        "In this homework, we will train a convolution neural network to recognize gesture Rock/Paper/Scissors. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PovEuTtBxGak",
        "colab_type": "text"
      },
      "source": [
        "## Load Rock/Paper/Scissors Dataset\n",
        "First we download the data and put them into its own directories for model training and evaluation. There are two dataset in zip format. Download each and put them into the corresponding directories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it1c0jCiNCIM",
        "colab_type": "code",
        "outputId": "540000d7-e2c0-486d-a5a3-97f9c8c7d214",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip \\\n",
        "    -O /tmp/rps.zip\n",
        "  \n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip \\\n",
        "    -O /tmp/rps-test-set.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-28 06:37:33--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.141.128, 2607:f8b0:400c:c06::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.141.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 200682221 (191M) [application/zip]\n",
            "Saving to: ‘/tmp/rps.zip’\n",
            "\n",
            "/tmp/rps.zip        100%[===================>] 191.38M   107MB/s    in 1.8s    \n",
            "\n",
            "2019-05-28 06:37:35 (107 MB/s) - ‘/tmp/rps.zip’ saved [200682221/200682221]\n",
            "\n",
            "--2019-05-28 06:37:36--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.141.128, 2607:f8b0:400c:c06::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.141.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29516758 (28M) [application/zip]\n",
            "Saving to: ‘/tmp/rps-test-set.zip’\n",
            "\n",
            "/tmp/rps-test-set.z 100%[===================>]  28.15M   145MB/s    in 0.2s    \n",
            "\n",
            "2019-05-28 06:37:36 (145 MB/s) - ‘/tmp/rps-test-set.zip’ saved [29516758/29516758]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnYP_HhYNVUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.14\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/rps.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/')\n",
        "zip_ref.close()\n",
        "\n",
        "local_zip = '/tmp/rps-test-set.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ5KIylxxoaL",
        "colab_type": "text"
      },
      "source": [
        "After running the folloiwng code, you will have exactly 840 images for each gesture. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrxdR83ANgjS",
        "colab_type": "code",
        "outputId": "3e97a034-18f7-415c-9162-4b602d8e3daa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "rock_dir = os.path.join('/tmp/rps/rock')\n",
        "paper_dir = os.path.join('/tmp/rps/paper')\n",
        "scissors_dir = os.path.join('/tmp/rps/scissors')\n",
        "\n",
        "print('total training rock images:', len(os.listdir(rock_dir)))\n",
        "print('total training paper images:', len(os.listdir(paper_dir)))\n",
        "print('total training scissors images:', len(os.listdir(scissors_dir)))\n",
        "\n",
        "rock_files = os.listdir(rock_dir)\n",
        "print(rock_files[:10])\n",
        "\n",
        "paper_files = os.listdir(paper_dir)\n",
        "print(paper_files[:10])\n",
        "\n",
        "scissors_files = os.listdir(scissors_dir)\n",
        "print(scissors_files[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total training rock images: 124\n",
            "total training paper images: 124\n",
            "total training scissors images: 124\n",
            "['testrock01-22.png', 'testrock02-20.png', 'testrock01-21.png', 'testrock02-17.png', 'testrock02-19.png', 'testrock04-25.png', 'testrock02-29.png', 'testrock03-26.png', 'testrock02-27.png', 'testrock04-05.png']\n",
            "['testpaper01-26.png', 'testpaper03-06.png', 'testpaper03-09.png', 'testpaper04-12.png', 'testpaper04-28.png', 'testpaper03-14.png', 'testpaper01-09.png', 'testpaper01-08.png', 'testpaper04-11.png', 'testpaper03-22.png']\n",
            "['testscissors01-09.png', 'testscissors03-11.png', 'testscissors01-24.png', 'testscissors02-11.png', 'testscissors03-17.png', 'testscissors02-29.png', 'testscissors04-14.png', 'testscissors01-19.png', 'testscissors02-02.png', 'testscissors04-29.png']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TsXh9cKxy2S",
        "colab_type": "text"
      },
      "source": [
        "Now, let's plot 2 images from each class.  You wil find those images are taken from different views and images vary a lot even for the same gesture. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp9dLel9N9DS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "pic_index = 2\n",
        "\n",
        "next_rock = [os.path.join(rock_dir, fname) for fname in rock_files[pic_index-2:pic_index]]\n",
        "next_paper = [os.path.join(paper_dir, fname) for fname in paper_files[pic_index-2:pic_index]]\n",
        "next_scissors = [os.path.join(scissors_dir, fname) for fname in scissors_files[pic_index-2:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(next_rock+next_paper+next_scissors):\n",
        "  print(img_path)\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "  plt.axis('Off')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV__GN11yO2P",
        "colab_type": "text"
      },
      "source": [
        "## TASK 1: Build a fully connect Neural Network\n",
        "First, let's try what we've learned from the previous lecture. We will build a FULLY connect neural networks to classify the gesture images.You are free to experiment with different structure of the network, data augmentation, dropout, different optimizer, and etc, to try to achieve the best performance on the TEST data in terms of accuracy.  Watch out for overfitting.\n",
        "\n",
        "Note that you should set test aside when you train your model. In the end, please report your model accuracy on the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWTisYLQM1aM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40cMjm9K06YE",
        "colab_type": "text"
      },
      "source": [
        "## TASK 2: Build Convolution Neural Network\n",
        "Now, let's try a convolution neural network (CNN) and see if we can achieve better performance. Similarly you are free to experiment with different structure of the network, techniques to avoid overfitting, different optimizer, and etc, to try to achieve the best performance on the TEST data in terms of accuracy.\n",
        "\n",
        "Note that you should set test aside when you train your model. In the end, please report your model accuracy on the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcxNZCS_090n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUuqO-p-yrMV",
        "colab_type": "text"
      },
      "source": [
        "## Use the best model to classify gestures\n",
        "You can now run the following code and use the model you trained to classify images uploaded from your laptop. Let us know how your model performs on the new unseen images.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZABJp7T3VLCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = fn\n",
        "  img = image.load_img(path, target_size=(150, 150))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(fn)\n",
        "  print(classes)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}