{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MSIS579-Lab4-RNN-IMDB-worksheet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uP-Kv6aIMrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.14\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "import keras\n",
        "keras.__version__\n",
        "\n",
        "!pip install numpy==1.16.1\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqA9pV73HoUg",
        "colab_type": "text"
      },
      "source": [
        "# MSIS579 Lab4: Recurrent Neural Networks to Movie Review Sentiment Analysis\n",
        "\n",
        "In this Lab, we will revisit the IMDB movie review dataset and apply Recurrent Neural Network to predict the sentiment of each review. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgJt_Q8iHoUi",
        "colab_type": "text"
      },
      "source": [
        "## Learning word embeddings with the `Embedding` layer\n",
        "\n",
        "Another popular and powerful way to associate a vector with a word is the use of dense \"word vectors\", also called \"word embeddings\". \n",
        "While the vectors obtained through one-hot encoding are binary, sparse (mostly made of zeros) and very high-dimensional (same dimensionality as the \n",
        "number of words in the vocabulary), \"word embeddings\" are low-dimensional floating point vectors \n",
        "(i.e. \"dense\" vectors, as opposed to sparse vectors). \n",
        "Unlike word vectors obtained via one-hot encoding, word embeddings are learned from data. \n",
        "It is common to see word embeddings that are 256-dimensional, 512-dimensional, or 1024-dimensional when dealing with very large vocabularies. \n",
        "On the other hand, one-hot encoding words generally leads to vectors that are 20,000-dimensional or higher (capturing a vocabulary of 20,000 \n",
        "token in this case). So, word embeddings pack more information into far fewer dimensions. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnhePEwoHoUh",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://s3.amazonaws.com/book.keras.io/img/ch6/word_embeddings.png\" alt=\"word embeddings vs. one hot encoding\" width=\"400\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNUkdDUlHoUi",
        "colab_type": "text"
      },
      "source": [
        "We can learn the word embeddings jointly with the main task you care about (e.g. document classification or sentiment prediction). In this setup, you would start with random word vectors, then learn your word vectors in the same way that you learn the weights of a neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0PTP5CKHoUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Embedding\n",
        "\n",
        "# The Embedding layer takes at least two arguments:\n",
        "# the number of possible tokens, here 1000 (1 + maximum word index),\n",
        "# and the dimensionality of the embeddings, here 64.\n",
        "embedding_layer = Embedding(1000, 64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgFk2dBkHoUl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The `Embedding` layer is best understood as a dictionary mapping integer indices (which stand for specific words) to dense vectors. It takes \n",
        "as input integers, it looks up these integers into an internal dictionary, and it returns the associated vectors. It's effectively a dictionary lookup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxYj8OkJHoUl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The `Embedding` layer takes as input a 2D tensor of integers, of shape `(samples, sequence_length)`, where each entry is a sequence of \n",
        "integers. It can embed sequences of variable lengths, so for instance we could feed into our embedding layer above batches that could have \n",
        "shapes `(32, 10)` (batch of 32 sequences of length 10) or `(64, 15)` (batch of 64 sequences of length 15). All sequences in a batch must \n",
        "have the same length, though (since we need to pack them into a single tensor), so sequences that are shorter than others should be padded \n",
        "with zeros, and sequences that are longer should be truncated.\n",
        "\n",
        "This layer returns a 3D floating point tensor, of shape `(samples, sequence_length, embedding_dimensionality)`. Such a 3D tensor can then \n",
        "be processed by a RNN layer or a 1D convolution layer (both will be introduced in the next sections).\n",
        "\n",
        "When you instantiate an `Embedding` layer, its weights (its internal dictionary of token vectors) are initially random, just like with any \n",
        "other layer. During training, these word vectors will be gradually adjusted via backpropagation, structuring the space into something that the \n",
        "downstream model can exploit. Once fully trained, your embedding space will show a lot of structure -- a kind of structure specialized for \n",
        "the specific problem you were training your model for.\n",
        "\n",
        "Let's apply this idea to the IMDB movie review sentiment prediction task that you are already familiar with. Let's quickly prepare \n",
        "the data. We will restrict the movie reviews to the top 10,000 most common words (like we did the first time we worked with this dataset), \n",
        "and cut the reviews after only 20 words. Our network will simply learn 8-dimensional embeddings for each of the 10,000 words, turn the \n",
        "input integer sequences (2D integer tensor) into embedded sequences (3D float tensor), flatten the tensor to 2D, and train a single `Dense` \n",
        "layer on top for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sJPeXcnHoUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras import preprocessing\n",
        "\n",
        "# Number of words to consider as features\n",
        "max_features = 10000\n",
        "# Cut texts after this number of words \n",
        "# (among top max_features most common words)\n",
        "maxlen = 20\n",
        "\n",
        "# Load the data as lists of integers.\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# TODO: turn our lists of integers into a 2D integer tensor of shape `(samples, maxlen)`\n",
        "x_train = None\n",
        "x_test = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MROjQpoiHoUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# TODO: add embedding layer\n",
        "# We specify the maximum input length to our Embedding layer\n",
        "# so we can later flatten the embedded inputs\n",
        "\n",
        "\n",
        "# TODO: add flatten layer\n",
        "# After the Embedding layer, our activations have shape `(samples, maxlen, 8)`.\n",
        "# We flatten the 3D tensor of embeddings into a 2D tensor of shape `(samples, maxlen * 8)`\n",
        "\n",
        "\n",
        "# TODO: add the final sigmoid layer to output the binary prediction\n",
        "# We add the classifier on top\n",
        "\n",
        "\n",
        "# TODO: compile the model and output the model summary\n",
        "\n",
        "\n",
        "# TODO: start training the model\n",
        "history = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUcxwPlnHoUr",
        "colab_type": "text"
      },
      "source": [
        "We get to a validation accuracy of ~76%, which is pretty good considering that we only look at the first 20 words in every review. But \n",
        "note that merely flattening the embedded sequences and training a single `Dense` layer on top leads to a model that treats each word in the \n",
        "input sequence separately, without considering inter-word relationships and structure sentence (e.g. it would likely treat both _\"this movie \n",
        "is shit\"_ and _\"this movie is the shit\"_ as being negative \"reviews\"). It would be much better to add recurrent layers or 1D convolutional \n",
        "layers on top of the embedded sequences to learn features that take into account each sequence as a whole. That's what we will focus on in \n",
        "the next few sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAANamq9IMrL",
        "colab_type": "text"
      },
      "source": [
        "## Recurrent Neural Networks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq_ynn7SIMrO",
        "colab_type": "text"
      },
      "source": [
        "In Keras, RNN can be invoked by the API `SimpleRNN`, which takes inputs of shape `(batch_size, timesteps, input_features)`.\n",
        "\n",
        "Like all recurrent layers in Keras, `SimpleRNN` can be run in two different modes: it can return either the full sequences of successive \n",
        "outputs for each timestep (a 3D tensor of shape `(batch_size, timesteps, output_features)`), or it can return only the last output for each \n",
        "input sequence (a 2D tensor of shape `(batch_size, output_features)`). These two modes are controlled by the `return_sequences` constructor \n",
        "argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX8XZQroIMrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 32))\n",
        "model.add(SimpleRNN(32))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le_b8-_9IMrR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(10000, 32))\n",
        "model.add(SimpleRNN(32, return_sequences=True))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t19y2RVCIMrU",
        "colab_type": "text"
      },
      "source": [
        "It is sometimes useful to stack several recurrent layers one after the other in order to increase the representational power of a network. \n",
        "In such a setup, you have to get all intermediate layers to return full sequences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs3_CaX0IMrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(10000, 32))\n",
        "model.add(SimpleRNN(32, return_sequences=True))\n",
        "model.add(SimpleRNN(32, return_sequences=True))\n",
        "model.add(SimpleRNN(32, return_sequences=True))\n",
        "model.add(SimpleRNN(32))  # This last layer only returns the last outputs.\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFa5RnFfIMrX",
        "colab_type": "text"
      },
      "source": [
        "Now let's try to use such a model on the IMDB movie review classification problem. First, let's preprocess the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUQZyM1rIMrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "max_features = 10000  # number of words to consider as features\n",
        "maxlen = 500  # cut texts after this number of words (among top max_features most common words)\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(input_train), 'train sequences')\n",
        "print(len(input_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
        "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
        "print('input_train shape:', input_train.shape)\n",
        "print('input_test shape:', input_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vkHIdmwIMra",
        "colab_type": "text"
      },
      "source": [
        "Let's train a simple recurrent network using an `Embedding` layer and a `SimpleRNN` layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HMFJak0IMrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, SimpleRNN\n",
        "\n",
        "# TODO: train a simple recurrent network using an Embedding layer and a SimpleRNN layer\n",
        "model = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgOT4hbdIMrd",
        "colab_type": "text"
      },
      "source": [
        "Let's display the training and validation loss and accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGra00XTIMre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ld5gB0XIMrh",
        "colab_type": "text"
      },
      "source": [
        "Our small recurrent network does better than the 1-layer NN with a 85% validation accuracy. Part of the problem is that `SimpleRNN` isn't very good at processing long sequences, like text. Other types of recurrent layers perform much better. Let's take a look at some \n",
        "more advanced layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOU0al5RIMri",
        "colab_type": "text"
      },
      "source": [
        "## Long short-term memory (LSTM)\n",
        "\n",
        "Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning.\n",
        "\n",
        "Now let's switch to more practical concerns: we will set up a model using a LSTM layer and train it on the IMDB data. Here's the network, similar to the one with `SimpleRNN` that we just presented. We only specify the output dimensionality of the LSTM layer, and leave every \n",
        "other argument (there are lots) to the Keras defaults. Keras has good defaults, and things will almost always \"just work\" without you having to spend time tuning parameters by hand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk5l4qh0IMri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import LSTM\n",
        "\n",
        "# TODO: train a simple recurrent network using an Embedding layer and a SimpleRNN layer\n",
        "model = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5uLLOc0IMrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSxtSWM2AYvu",
        "colab_type": "text"
      },
      "source": [
        "## Extract the word embedding from RNN\n",
        "\n",
        "Finally, let's take a look at the word embedding learnt from the IMDB movie review dataset. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYgNbASVAfs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# access the embedding layer through the constructed model \n",
        "# first `0` refers to the position of embedding layer in the `model`\n",
        "# `embeddings` has a shape of (num_vocab, embedding_dim) \n",
        "embeddings = None\n",
        "\n",
        "# `word_to_index` is a mapping (i.e. dict) from words to their index, e.g. `love`: 69\n",
        "words_embeddings = None\n",
        "\n",
        "# now you can use it like this for example\n",
        "print(words_embeddings['great'])\n",
        "print(words_embeddings['good'])\n",
        "print(words_embeddings['bad'])\n",
        "\n",
        "print(\"Similarity between good and great: \" + str(cosine_similarity([words_embeddings['good'], words_embeddings['great']])[0,1]))\n",
        "print(\"Similarity between good and bad: \" + str(cosine_similarity([words_embeddings['good'], words_embeddings['bad']])[0,1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Yo_iK4U_CET",
        "colab_type": "text"
      },
      "source": [
        "# End of Lab4"
      ]
    }
  ]
}